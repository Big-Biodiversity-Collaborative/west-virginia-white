---
title: "West Virginia White"
author: "Jeff Oliver"
date: "`r format(Sys.time(), '%d %B, %Y')`"
output: pdf_document
---

<!--
TODO: Need to consider sampling only a portion for each year.
-->

```{r setup, include = FALSE}
knitr::opts_chunk$set(echo = FALSE)
suppressPackageStartupMessages(library(tidyverse))
suppressPackageStartupMessages(library(lubridate))
suppressPackageStartupMessages(library(ggplot2))
suppressPackageStartupMessages(library(gridExtra))
suppressPackageStartupMessages(library(parallel))

# Establish bootstrap and sampling values
minimum.required <- 5
sample.size <- minimum.required
num.bs.reps <- 100
```

## Preliminary analyses

"Preliminary"" does not do it justice. This is very, very back of the envelope. Data are from iNaturalist observations downloaded on 2 May 2019 and GBIF data downloaded on 17 May 2019.

```{r load-prepare-data}
# Read in iNaturalist observations
# 52539 is West Virginia White
iNaturalist <- read.csv(file = "data/observations-52593.csv")
iNat.obs <- iNaturalist[, c("latitude", "longitude", "observed_on")]
iNat.obs$observed_on <- as.Date(iNat.obs$observed_on)

# Read in GBIF observations
gbif <- read.delim(file = "data/0015022-190415153152247.csv")
gbif.obs <- gbif[, c("decimalLatitude", "decimalLongitude", "eventDate")]
gbif.obs$eventDate <- as.Date(x = gbif.obs$eventDate)
gbif.obs <- na.omit(gbif.obs)

# Join data sets
colnames(iNat.obs) <- c("latitude", "longitude", "date")
colnames(gbif.obs) <- c("latitude", "longitude", "date")
wvw <- rbind(iNat.obs, gbif.obs)

# Remove some bad geo coordinates (mostly GBIF)
wvw <- wvw[wvw$latitude > 29, ]
wvw <- wvw[wvw$longitude > -100, ]

# Extract year and day of year
wvw$year <- year(wvw$date)
wvw$yday <- as.POSIXlt(wvw$date)$yday

# Dates of January 1 (yday == 0) are not realistic
wvw <- wvw[wvw$yday > 0, ]
# Drop any from current, incomplete year
wvw <- wvw[wvw$year != year(today()), ]

# Drop duplicates
wvw <- unique(wvw)
```

After dropping some unrealistic GBIF observations (some from the Indian Ocean some from January 1) and those from `r year(today())`, there are `r nrow(wvw)` observations. If we plot these by year and day of year, it looks like recently there may have been a shift to earlier emergences.

```{r plot-data}
# Plot observations for each year separately
obs.plot <- ggplot(data = wvw, mapping = aes(x = year, y = yday, color = latitude)) +
  geom_point() +
  geom_smooth() +
  xlab(label = "Year") +
  scale_color_gradient(name = "Latitude") +
  ylab(label = "Day") +
  theme_bw() +
  theme(legend.position = c(0.15, 0.75),
        plot.title = element_text(size = 10)) +
  ggtitle("P. virginiensis observations")

# Plot _only_ minimums
wvw.mins <- wvw %>%
  group_by(year) %>%
  summarise(early = min(yday, na.rm = TRUE),
            n.obs = n())

min.plot.title <- paste0("Earliest observations, ", 
                     min(wvw$year), " - ", 
                     max(wvw$year))
min.plot <- ggplot(data = wvw.mins, mapping = aes(x = year, y = early)) +
  geom_point() +
  geom_smooth()  +
  xlab(label = "Year") +
  ylab(label = "Day") +
  ggtitle(min.plot.title) + 
  theme_bw() +
  theme(plot.title = element_text(size = 10))

# Plot the two plots, suppressing the smoothing message that prints out
suppressMessages(expr = grid.arrange(obs.plot, min.plot, nrow = 1))
```

Note that observations from lower latitudes (darker points) are generally towards the bottom of the plot, and observations from more northern latitudes (lighter points) are nearer the top of the plot. No big surprise there.

There _is_ a trend towards earlier emergences in more recent years, so let's consider a very crude linear model:
$$
Earliest \enspace observation \enspace day_i = Year_i + \epsilon_i
$$

```{r minimums-model}
# Linear regression 
early.lm <- lm(early ~ year, data = wvw.mins)
early.summary <- summary(early.lm)
year.coeff <- early.summary$coefficients[2, 1]
year.p <- early.summary$coefficients[2, 4]
```

There is an effect of year on earliest observation date, with the first earliest observation occuring `r abs(round(year.coeff, digits = 2))` days earlier each year (p = `r round(year.p, digits = 4)`). Cool!

## But...

Given the increase in butterfly watching, this change in observations could be entirely due to sampling artifacts than biological reality. Consider the number of observations of _P. virginiensis_ through time:

```{r wvw-obs-plot, fig.height = 3}
wvw.plot.title <- paste0("Number of observations per year, ", 
                     min(wvw$year), " - ", 
                     max(wvw$year))

wvw.plot <- ggplot(data = wvw.mins, mapping = aes(x = year, y = n.obs)) +
  geom_point() +
  geom_smooth()  +
  xlab(label = "Year") +
  ylab(label = "Observations") +
  ggtitle(wvw.plot.title) + 
  theme_bw() + 
  theme(plot.title = element_text(size = 10))

suppressMessages(expr = print(wvw.plot))
```

Pretty clearly increasing. So this means that by chance, recent years are more likely to "catch" earlier observations, just because there are more opportunities. To see this in action, consider a thought experiment where we make up data. Well, bootstrapping data, but it's nearly the same thing. If we create a data set that mimics the observation efforts for the observed data (i.e. `r wvw.mins$n.obs[1]` in `r wvw.mins$year[1]`, `r wvw.mins$n.obs[floor(nrow(wvw.mins)/2)]` in `r wvw.mins$year[floor(nrow(wvw.mins)/2)]`, `r wvw.mins$n.obs[nrow(wvw.mins)]` in `r wvw.mins$year[nrow(wvw.mins)]`, etc.), but instead of actual observations, sample only from the most recent year of observations (`r wvw.mins$year[nrow(wvw.mins)]`). We then use those data to run the linear regression again and see if there is an effect. Ideally, if there is _no_ artifact of sampling, we should see, on average, no effect of year on earliest observation (this is because, for these data, _all_ days of observation are being drawn from the "real" data for `r wvw.mins$year[nrow(wvw.mins)]` alone). 

```{r demonstrate-artifact, fig.height = 3}
sample.from <- wvw$yday[wvw$year == 2018]
include.years <- wvw.mins$year[wvw.mins$n.obs >= minimum.required]
# A data frame we'll use as template for storing bootstrapped data
bootstrapped.df <- data.frame(year = rep(include.years, times = sample.size),
                                yday = NA)

# Set up parallel processing
num.cores <- detectCores() - 1
# Initiate cluster
the.cluster <- makeCluster(num.cores)

# Need to explicitly load packages in cluster; invisible the only way to prevent
# output created by loading tidyverse on cores
invisible(x = clusterEvalQ(cl = the.cluster, { library(tidyverse) }))

bs.results.ideal <- parLapply(cl = the.cluster,
                        X = c(1:num.bs.reps), # First argument of fun
                        fun = function(i, bs.df, sample.vec, sample.size) {
                          # Fill in data frame with bootstrapped date for each year
                          for (yr in unique(bs.df$year)) {
                            bs.df$yday[bs.df$year == yr] <- sample(x = sample.vec,
                                                                   size = sample.size,
                                                                   replace = FALSE)
                          }
                          bs.mins <- bs.df %>%
                            group_by(year) %>%
                            summarise(early = min(yday))
                          
                          bs.lm <- lm(early ~ year, data = bs.mins)
                          coeff.est <- summary(bs.lm)$coefficients[2, 1]
                          p.value <- summary(bs.lm)$coefficients[2, 4]
                          return(data.frame(bs.rep = i,
                                            estimate = coeff.est,
                                            p.value = p.value))
                          
                        }, # end of parLapply fun
                        bs.df = bootstrapped.df, # Three additional parameters passed to fun
                        sample.vec = sample.from,
                        sample.size = sample.size)
stopCluster(the.cluster)
# Need to convert vanilla list into data frame
bs.results.ideal <- do.call(rbind, bs.results.ideal)

########################################
# Bootstrap and demonstrate the artifact
# A data frame we'll use as template for storing bootstrapped data
bootstrapped.df <- wvw[wvw$year %in% include.years, ]

# Initiate cluster
the.cluster <- makeCluster(num.cores)

# Need to explicitly load packages in cluster; invisible the only way to prevent
# output created by loading tidyverse on cores
invisible(x = clusterEvalQ(cl = the.cluster, { library(tidyverse) }))

bs.results.artifact <- parLapply(cl = the.cluster,
                        X = c(1:num.bs.reps), # First argument of fun
                        fun = function(i, bs.df, sample.vec) {
                          # Fill in data frame with bootstrapped date for each year
                          for (yr in unique(bs.df$year)) {
                            num.obs <- sum(bs.df$year == yr)
                            bs.df$yday[bs.df$year == yr] <- sample(x = sample.vec,
                                                                   size = num.obs)
                          }
                          bs.mins <- bs.df %>%
                            group_by(year) %>%
                            summarise(early = min(yday))
                          
                          suppressMessages(expr = bs.lm <- lm(early ~ year, data = bs.mins))
                          coeff.est <- summary(bs.lm)$coefficients[2, 1]
                          p.value <- summary(bs.lm)$coefficients[2, 4]
                          return(data.frame(bs.rep = i,
                                            estimate = coeff.est,
                                            p.value = p.value))
                          
                        }, # end of parLapply fun
                        bs.df = bootstrapped.df, # Three additional parameters passed to fun
                        sample.vec = sample.from) # End of parLapply call
stopCluster(the.cluster)
# Need to convert vanilla list into data frame
bs.results.artifact <- do.call(rbind, bs.results.artifact)

# Plot of results of bootstrapping, in ideal world
estimate.ideal.plot <- ggplot(data = bs.results.ideal, mapping = aes(x = estimate)) +
  geom_histogram() +
  geom_vline(xintercept = 0) +
  xlab(label = "Effect size (days/year)") +
  ylab(label = "Count") +
  ggtitle(label = "Expectation in artifact-free world") +
  theme_bw() +
  theme(plot.title = element_text(size = 10))

# Plot of results of bootstrapping, demonstrating artifact
estimate.artifact.plot <- ggplot(data = bs.results.artifact, mapping = aes(x = estimate)) +
  geom_histogram() +
  geom_vline(xintercept = 0, color = "#FF0000") +
  xlab(label = "Effect size (days/year)") +
  ylab(label = "Count") +
  ggtitle(label = "Artifact caused by changes in effort") +
  theme_bw() +
  theme(plot.title = element_text(size = 10))

# Two-panel plot of bootstrapping results
suppressMessages(expr = grid.arrange(estimate.ideal.plot, estimate.artifact.plot, nrow = 1))
```

Repeating this process `r num.bs.reps` times should result, on average, of an effect size of 0 (left panel). However, when we do the bootstrapping experiment, it looks like there is considerable potential for an artifact (right panel). The mean effect size from the right is `r round(mean(bs.results.artifact$estimate, na.rm = TRUE), digits = 2)`, which we would take to mean that the first observation is getting earlier by `r abs(round(mean(bs.results.artifact$estimate, na.rm = TRUE), digits = 2))` days per year. We know this is an artifact because all the data are based on `r wvw.mins$year[nrow(wvw.mins)]`.

## Back to the bootstrap

However, we can use bootstrapping to down-sample observations to make effort across years consistent. That is, for each year, we randomly sample a subset of observations so we only have a certain number of observations per year. For that "certain number", we'll a minimum of `r minimum.required` observations per year. Let's test this first by doing the same process we ran before, basing everying on data from `r wvw.mins$year[nrow(wvw.mins)]` alone, but now only drawing `r sample.size` samples for each year. Ideally, we should see no effect of year on earliest observation (i.e. an effect size of 0).

```{r bootstrap-proof-of-concept, fig.height = 3}
sample.from <- wvw$yday[wvw$year == 2018]
include.years <- wvw.mins$year[wvw.mins$n.obs >= minimum.required]

# A data frame we'll use as template for storing bootstrapped data
bootstrapped.df <- data.frame(year = rep(include.years, times = sample.size),
                              yday = NA)

# Initiate cluster
the.cluster <- makeCluster(num.cores)

# Need to explicitly load packages in cluster; invisible the only way to prevent
# output created by loading tidyverse on cores
invisible(x = clusterEvalQ(cl = the.cluster, { library(tidyverse) }))

bs.results.downsample <- parLapply(cl = the.cluster,
                        X = c(1:num.bs.reps),
                        fun = function(i, bs.df, sample.vec, sample.size) {
                          # Fill in data bootstrapped for each year, sampling
                          # *only* from sample.vec, and restricting sample 
                          # size of each year to sample.size
                          for (yr in unique(bs.df$year)) {
                            bs.df$yday[bs.df$year == yr] <- sample(x = sample.vec,
                                                                   size = sample.size,
                                                                   replace = FALSE)
                          }
                          bs.mins <- bs.df %>%
                            group_by(year) %>%
                            summarise(early = min(yday))
                          
                          bs.lm <- lm(early ~ year, data = bs.mins)
                          coeff.est <- summary(bs.lm)$coefficients[2, 1]
                          p.value <- summary(bs.lm)$coefficients[2, 4]
                          return(data.frame(bs.rep = i,
                                            estimate = coeff.est,
                                            p.value = p.value))
                        },
                        bs.df = bootstrapped.df,
                        sample.vec = sample.from,
                        sample.size = sample.size)

stopCluster(the.cluster)
# Need to convert vanilla list into data frame
bs.results.downsample <- do.call(rbind, bs.results.downsample)

# Plot results of bootstrapping
# Coefficient estimates
estimate.plot <- ggplot(data = bs.results.downsample, mapping = aes(x = estimate)) +
  geom_histogram() +
  geom_vline(xintercept = 0) +
  xlab(label = "Effect size (days/year)") +
  ylab(label = "Count") +
  ggtitle(label = "Downsampling to avoid artifacts") + 
  theme_bw() +
  theme(plot.title = element_text(size = 10))

suppressMessages(expr = print(estimate.plot))
```

Woo-hoo! So now we have a way to avoid artifacts due to variation in effort. Let's try it for real, downsampling each years' data to only `r minimum.required` per year. Before we try that, what effect does this restriction of `r minimum.required` observations per year have on the size of our data set? We had `r nrow(wvw)` observations, but if we restrict it to only those years with at least `r minimum.required` observations, we have `r nrow(wvw[wvw$year %in% include.years, ])` total observations, spanning `r min(wvw$year[wvw$year %in% include.years])` through `r max(wvw$year[wvw$year %in% include.years])`. Taking a look at these data:

```{r reduced-data-plot, fig.height = 3}
# Plot only those years that have at least minimum.required number of observations
wvw.min.obs <- wvw[wvw$year %in% include.years, ]

obs.plot.title <- paste0("P. virginiensis, ",
                         min(wvw.min.obs$year), " - ", 
                         max(wvw.min.obs$year))
obs.plot <- ggplot(data = wvw.min.obs, mapping = aes(x = year, 
                                                     y = yday)) +
  geom_point() +
  geom_smooth() +
  xlab(label = "Year") +
  ylab(label = "Day") +
  ggtitle(obs.plot.title) +
  theme_bw() +
  theme(plot.title = element_text(size = 10))

# And plot number of observations per year
wvw.min.n.obs <- wvw.min.obs %>%
  group_by(year) %>%
  summarise(early = min(yday, na.rm = TRUE),
            n.obs = n())

wvw.plot.title <- paste0("Observations per year, ", 
                     min(wvw.min.n.obs$year), " - ", 
                     max(wvw.min.n.obs$year))

early.obs.plot <- ggplot(data = wvw.min.n.obs, mapping = aes(x = year, y = n.obs)) +
  geom_point() +
  geom_smooth()  +
  xlab(label = "Year") +
  ylab(label = "# Observations") +
  ggtitle(wvw.plot.title) + 
  theme_bw() +
  theme(plot.title = element_text(size = 10))

suppressMessages(expr = grid.arrange(obs.plot, early.obs.plot, nrow = 1))
```

There is still an increase in number of observations per year (right panel), so we need apply the downsampling approach to avoid the artifact described above. Downsampling to include only `r sample.size` samples from each year is going to vary each time we do a bootstrapping event. To see this in action, the plots below show three iterations, with different points being sampled at each iteration.

```{r downsample-in-action, fig.height = 3}
# Do three iterations of downsampling and plot these as we have before (all 
# observations, then only the earliest observations)
include.years <- wvw.mins$year[wvw.mins$n.obs >= minimum.required]
wvw.min.obs <- wvw[wvw$year %in% include.years, ]
num.iterations <- 3

# A data frame with yday empty to be filled in
downsample.df <- data.frame(iteration = rep(x = c(1:3), 
                                             times = sample.size * length(include.years)),
  year = rep(include.years, times = num.iterations * sample.size),
                             yday = NA)
for (i in 1:num.iterations) {
  for (yr in unique(x = downsample.df$year)) {
    downsample.df$yday[downsample.df$iteration == i & downsample.df$year == yr] <-
      sample(x = wvw.min.obs$yday[wvw.min.obs$year == yr], 
             size = sample.size, 
             replace = FALSE)
  }
}

# Ensure iteration gets treated as factor
downsample.df$iteration <- as.factor(downsample.df$iteration)

# Extract minimums
downsample.mins <- downsample.df %>%
  group_by(iteration, year) %>%
  summarise(early = min(yday, na.rm = TRUE),
            n.obs = n())

# Create two plots, all observations and earliest observations by year, coloring
# each iteration separately
# All observations
downsample.obs.plot <- ggplot(data = downsample.df, mapping = aes(x = year,
                                                                   y = yday,
                                                                   group = iteration,
                                                                   color = iteration)) +
  geom_point() +
  geom_smooth() +
  xlab(label = "Year") +
  ylab(label = "Day") +
  ggtitle(label = "Downsampled observations") +
  theme_bw() +
  theme(plot.title = element_text(size = 10),
        legend.position = "none")

# Only earliest date per year
downsample.min.plot <- ggplot(data = downsample.mins, mapping = aes(x = year,
                                                                    y = early,
                                                                    group = iteration,
                                                                    color = iteration)) +
  geom_point() +
  geom_smooth() +
  ylim(c(min(downsample.df$yday), max(downsample.df$yday))) +
  xlab(label = "Year") +
  ylab(label = "Day") +
  ggtitle(label = "Downsampled early dates") +
  theme_bw() +
  scale_color_discrete(name = "Iteration") +
  theme(plot.title = element_text(size = 10),
        legend.position = c(0.8, 0.8),
        legend.background = element_rect(size = 1),
        legend.title = element_text(size = 8),
        legend.text = element_text(size = 8),
        legend.key.size = unit(x = 0.5, units = "line"))

suppressMessages(expr = grid.arrange(downsample.obs.plot, downsample.min.plot, nrow = 1))
```

### Moment of truth

```{r downsample-bootstrap, fig.height = 3}

```
